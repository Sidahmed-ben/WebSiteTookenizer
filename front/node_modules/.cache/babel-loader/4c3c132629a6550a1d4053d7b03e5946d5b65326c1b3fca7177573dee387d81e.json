{"ast":null,"code":"var EventEmitter = require('events').EventEmitter;\nvar util = require('util');\nvar assert = require('assert');\nvar Transform = require('stream').Transform;\nvar disect = require('disect');\nfunction noop() {}\nfunction Tokenizer(check_token_cb, options) {\n  if (!(this instanceof Tokenizer)) {\n    return new Tokenizer(check_token_cb);\n  }\n  Transform.call(this, options);\n  this._readableState.objectMode = true;\n  this._buffered = \"\"; // we buffer untokenized data between writes\n  this._regexes = []; // should contain objects \n  // with regex[RegExp] and type[String]\n  this._ignored = {}; // a hash of ignored token types\n  // these will be parsed but not emitted\n  this._checkToken = check_token_cb || noop;\n}\nutil.inherits(Tokenizer, Transform);\nTokenizer.prototype._transform = function _transform(chunk, encoding, callback) {\n  chunk = chunk.toString();\n  var self = this;\n  process.nextTick(function () {\n    try {\n      var index = 0,\n        step = 64;\n      while (index < chunk.length) {\n        self._tokenize(chunk.substr(index, step));\n        index += step;\n      }\n      callback();\n    } catch (e) {\n      callback(e);\n    }\n  });\n};\nTokenizer.prototype._getMatchingRule = function _getMatchingRule(str) {\n  for (var i = 0; i < this._regexes.length; ++i) {\n    if (this._regexes[i].regex.test(str)) {\n      return this._regexes[i];\n    }\n  }\n  return null;\n};\nTokenizer.prototype._tokenize = function _tokenize(data, nobuffer) {\n  var regexes = this._regexes;\n  // in case we buffered data on previous writes\n  data = this._buffered + data;\n  this._buffered = '';\n  if (!data.length) {\n    return;\n  }\n  var self = this;\n  var maxIndex = disect(0, data.length, function (index) {\n    var buf = data.substring(0, index + 1);\n    return self._getMatchingRule(buf) === null;\n  });\n  if (maxIndex === 0) {\n    // no match found\n    throw new SyntaxError('could not tokenize ' + JSON.stringify(data));\n  } else if (maxIndex === data.length && !nobuffer) {\n    // the whole string is matching\n    this._buffered = data;\n    return;\n  } else {\n    // some substring is matching\n    var str = data.substring(0, maxIndex);\n    var rule = this._getMatchingRule(str);\n    if (!rule) {\n      throw new Error('wut ?');\n    }\n    this._gotToken(str, rule);\n    this._tokenize(data.substring(maxIndex), nobuffer);\n  }\n};\nTokenizer.prototype._flush = function _flush(callback) {\n  var self = this;\n  process.nextTick(function () {\n    try {\n      self._tokenize('', true);\n      callback();\n    } catch (e) {\n      callback(e);\n    }\n  });\n};\nvar Token = function String(content, type) {\n  this.content = content;\n  this.type = type;\n  this.toString = function () {\n    return this.content.toString();\n  };\n};\nutil.inherits(Token, String);\nToken.prototype.valueOf = function valueOf() {\n  return this.content;\n};\nTokenizer.prototype._gotToken = function _gotToken(str, rule) {\n  // notify the token checker\n  var type = this._checkToken(str, rule) || rule.type;\n  if (this._ignored[type]) return;\n  var token = new Token(str, type);\n  this.push(token);\n  this.emit('token', token, type);\n};\nTokenizer.prototype.addRule = function addRule(regex, type) {\n  // this is useful for built-in rules\n  if (!type) {\n    if (Array.isArray(regex)) {\n      return this.addRule(regex[0], regex[1]);\n    } else if (regex) {\n      return this.addRule(Tokenizer[regex]);\n    } else {\n      throw new Error('No parameters specified');\n    }\n  }\n  assert.ok(regex instanceof RegExp || typeof regex === 'function');\n  assert.equal(typeof type, 'string');\n  this._regexes.push({\n    regex: regex,\n    type: type\n  });\n};\n\n/**\n * set some tokens to be ignored. these won't be emitted\n */\nTokenizer.prototype.ignore = function ignore(ignored) {\n  if (typeof ignore === 'array') {\n    for (var i = 0; i < ignored.length; ++i) {\n      this.ignore(ignored[i]);\n    }\n    return;\n  }\n  this._ignored[ignored] = true;\n};\nmodule.exports = Tokenizer;\n\n// built-in rules\nTokenizer.whitespace = [/^(\\s)+$/, 'whitespace'];\nTokenizer.word = [/^\\w+$/, 'word'];\nTokenizer.number = [/^\\d+(\\.\\d+)?$/, 'number'];","map":{"version":3,"names":["EventEmitter","require","util","assert","Transform","disect","noop","Tokenizer","check_token_cb","options","call","_readableState","objectMode","_buffered","_regexes","_ignored","_checkToken","inherits","prototype","_transform","chunk","encoding","callback","toString","self","process","nextTick","index","step","length","_tokenize","substr","e","_getMatchingRule","str","i","regex","test","data","nobuffer","regexes","maxIndex","buf","substring","SyntaxError","JSON","stringify","rule","Error","_gotToken","_flush","Token","String","content","type","valueOf","token","push","emit","addRule","Array","isArray","ok","RegExp","equal","ignore","ignored","module","exports","whitespace","word","number"],"sources":["/home/sidahmed/Bureau/Cours_Master_1/S2/Web/Partie2/MoteurRecherche/moteur-rech/node_modules/tokenizer/lib/Tokenizer.js"],"sourcesContent":["var EventEmitter = require('events').EventEmitter;\nvar util = require('util');\nvar assert = require('assert');\nvar Transform = require('stream').Transform;\nvar disect = require('disect');\n\nfunction noop(){}\n\nfunction Tokenizer (check_token_cb, options) {\n    if(!(this instanceof Tokenizer)) {\n      return new Tokenizer(check_token_cb);\n    }\n\n    Transform.call(this, options);\n    this._readableState.objectMode = true;\n    this._buffered = \"\"; // we buffer untokenized data between writes\n    this._regexes = []; // should contain objects \n                        // with regex[RegExp] and type[String]\n    this._ignored = {}; // a hash of ignored token types\n                        // these will be parsed but not emitted\n    this._checkToken = check_token_cb || noop;\n}\nutil.inherits(Tokenizer, Transform);\n\nTokenizer.prototype._transform = function _transform(chunk, encoding, callback) {\n  chunk = chunk.toString();\n  var self = this;\n  process.nextTick(function () {\n    try {\n      var index = 0, step = 64;\n      while(index < chunk.length) {\n        self._tokenize(chunk.substr(index, step));\n        index += step;\n      }\n      callback();\n    } catch(e) {\n      callback(e);\n    }\n  })\n};\n\nTokenizer.prototype._getMatchingRule = function _getMatchingRule(str) {\n  for (var i = 0; i < this._regexes.length; ++i) {\n      if(this._regexes[i].regex.test(str)) {\n        return this._regexes[i];\n      }\n  }\n  return null;\n};\n\nTokenizer.prototype._tokenize = function _tokenize(data, nobuffer) {\n    var regexes = this._regexes;\n    // in case we buffered data on previous writes\n    data = this._buffered + data;\n    this._buffered = '';\n    if(!data.length) {\n      return;\n    }\n\n    var self = this;\n    var maxIndex = disect(0, data.length, function (index) {\n      var buf = data.substring(0, index + 1);\n      return self._getMatchingRule(buf) === null;\n    });\n\n    if(maxIndex === 0) {\n      // no match found\n      throw new SyntaxError('could not tokenize ' + JSON.stringify(data));\n    }\n    else if (maxIndex === data.length && !nobuffer) {\n      // the whole string is matching\n      this._buffered = data;\n      return;\n    }\n    else {\n      // some substring is matching\n      var str = data.substring(0, maxIndex);\n      var rule = this._getMatchingRule(str);\n      if(!rule) {\n        throw new Error('wut ?');\n      }\n      this._gotToken(str, rule);\n      this._tokenize(data.substring(maxIndex), nobuffer);\n    }\n};\n\nTokenizer.prototype._flush = function _flush(callback) {\n  var self = this;\n  process.nextTick(function () {\n    try {\n      self._tokenize('', true);\n      callback();\n    } catch(e) {\n      callback(e);\n    }\n  });\n};\n\nvar Token = function String (content, type) {\n  this.content = content;\n  this.type = type;\n  this.toString = function () {\n    return this.content.toString();\n  }\n}\nutil.inherits(Token, String);\nToken.prototype.valueOf = function valueOf() {\n  return this.content;\n};\n\nTokenizer.prototype._gotToken = function _gotToken(str, rule) {\n    // notify the token checker\n    var type = this._checkToken(str, rule) || rule.type;\n    if(this._ignored[type]) return;\n    var token = new Token(str, type);\n\n    this.push(token);\n\n    this.emit('token', token, type);\n};\n\nTokenizer.prototype.addRule = function addRule(regex, type) {\n    // this is useful for built-in rules\n    if(!type) {\n      if(Array.isArray(regex)) {\n        return this.addRule(regex[0], regex[1]);\n      }\n      else if(regex) {\n        return this.addRule(Tokenizer[regex]);\n      }\n      else {\n        throw new Error('No parameters specified');\n      }\n    }\n    assert.ok((regex instanceof RegExp) || (typeof regex === 'function'));\n    assert.equal(typeof type, 'string');\n    this._regexes.push({regex:regex,type:type});\n};\n\n/**\n * set some tokens to be ignored. these won't be emitted\n */\nTokenizer.prototype.ignore = function ignore(ignored) {\n    if(typeof ignore === 'array') {\n        for (var i = 0; i < ignored.length; ++i) {\n            this.ignore(ignored[i]);\n        }\n        return;\n    }\n    this._ignored[ignored] = true;\n};\n\nmodule.exports = Tokenizer;\n\n// built-in rules\nTokenizer.whitespace    = [/^(\\s)+$/, 'whitespace'];\nTokenizer.word          = [/^\\w+$/, 'word'];\nTokenizer.number        = [/^\\d+(\\.\\d+)?$/, 'number'];\n"],"mappings":"AAAA,IAAIA,YAAY,GAAGC,OAAO,CAAC,QAAQ,CAAC,CAACD,YAAY;AACjD,IAAIE,IAAI,GAAGD,OAAO,CAAC,MAAM,CAAC;AAC1B,IAAIE,MAAM,GAAGF,OAAO,CAAC,QAAQ,CAAC;AAC9B,IAAIG,SAAS,GAAGH,OAAO,CAAC,QAAQ,CAAC,CAACG,SAAS;AAC3C,IAAIC,MAAM,GAAGJ,OAAO,CAAC,QAAQ,CAAC;AAE9B,SAASK,IAAIA,CAAA,EAAE,CAAC;AAEhB,SAASC,SAASA,CAAEC,cAAc,EAAEC,OAAO,EAAE;EACzC,IAAG,EAAE,IAAI,YAAYF,SAAS,CAAC,EAAE;IAC/B,OAAO,IAAIA,SAAS,CAACC,cAAc,CAAC;EACtC;EAEAJ,SAAS,CAACM,IAAI,CAAC,IAAI,EAAED,OAAO,CAAC;EAC7B,IAAI,CAACE,cAAc,CAACC,UAAU,GAAG,IAAI;EACrC,IAAI,CAACC,SAAS,GAAG,EAAE,CAAC,CAAC;EACrB,IAAI,CAACC,QAAQ,GAAG,EAAE,CAAC,CAAC;EACA;EACpB,IAAI,CAACC,QAAQ,GAAG,CAAC,CAAC,CAAC,CAAC;EACA;EACpB,IAAI,CAACC,WAAW,GAAGR,cAAc,IAAIF,IAAI;AAC7C;AACAJ,IAAI,CAACe,QAAQ,CAACV,SAAS,EAAEH,SAAS,CAAC;AAEnCG,SAAS,CAACW,SAAS,CAACC,UAAU,GAAG,SAASA,UAAUA,CAACC,KAAK,EAAEC,QAAQ,EAAEC,QAAQ,EAAE;EAC9EF,KAAK,GAAGA,KAAK,CAACG,QAAQ,EAAE;EACxB,IAAIC,IAAI,GAAG,IAAI;EACfC,OAAO,CAACC,QAAQ,CAAC,YAAY;IAC3B,IAAI;MACF,IAAIC,KAAK,GAAG,CAAC;QAAEC,IAAI,GAAG,EAAE;MACxB,OAAMD,KAAK,GAAGP,KAAK,CAACS,MAAM,EAAE;QAC1BL,IAAI,CAACM,SAAS,CAACV,KAAK,CAACW,MAAM,CAACJ,KAAK,EAAEC,IAAI,CAAC,CAAC;QACzCD,KAAK,IAAIC,IAAI;MACf;MACAN,QAAQ,EAAE;IACZ,CAAC,CAAC,OAAMU,CAAC,EAAE;MACTV,QAAQ,CAACU,CAAC,CAAC;IACb;EACF,CAAC,CAAC;AACJ,CAAC;AAEDzB,SAAS,CAACW,SAAS,CAACe,gBAAgB,GAAG,SAASA,gBAAgBA,CAACC,GAAG,EAAE;EACpE,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACrB,QAAQ,CAACe,MAAM,EAAE,EAAEM,CAAC,EAAE;IAC3C,IAAG,IAAI,CAACrB,QAAQ,CAACqB,CAAC,CAAC,CAACC,KAAK,CAACC,IAAI,CAACH,GAAG,CAAC,EAAE;MACnC,OAAO,IAAI,CAACpB,QAAQ,CAACqB,CAAC,CAAC;IACzB;EACJ;EACA,OAAO,IAAI;AACb,CAAC;AAED5B,SAAS,CAACW,SAAS,CAACY,SAAS,GAAG,SAASA,SAASA,CAACQ,IAAI,EAAEC,QAAQ,EAAE;EAC/D,IAAIC,OAAO,GAAG,IAAI,CAAC1B,QAAQ;EAC3B;EACAwB,IAAI,GAAG,IAAI,CAACzB,SAAS,GAAGyB,IAAI;EAC5B,IAAI,CAACzB,SAAS,GAAG,EAAE;EACnB,IAAG,CAACyB,IAAI,CAACT,MAAM,EAAE;IACf;EACF;EAEA,IAAIL,IAAI,GAAG,IAAI;EACf,IAAIiB,QAAQ,GAAGpC,MAAM,CAAC,CAAC,EAAEiC,IAAI,CAACT,MAAM,EAAE,UAAUF,KAAK,EAAE;IACrD,IAAIe,GAAG,GAAGJ,IAAI,CAACK,SAAS,CAAC,CAAC,EAAEhB,KAAK,GAAG,CAAC,CAAC;IACtC,OAAOH,IAAI,CAACS,gBAAgB,CAACS,GAAG,CAAC,KAAK,IAAI;EAC5C,CAAC,CAAC;EAEF,IAAGD,QAAQ,KAAK,CAAC,EAAE;IACjB;IACA,MAAM,IAAIG,WAAW,CAAC,qBAAqB,GAAGC,IAAI,CAACC,SAAS,CAACR,IAAI,CAAC,CAAC;EACrE,CAAC,MACI,IAAIG,QAAQ,KAAKH,IAAI,CAACT,MAAM,IAAI,CAACU,QAAQ,EAAE;IAC9C;IACA,IAAI,CAAC1B,SAAS,GAAGyB,IAAI;IACrB;EACF,CAAC,MACI;IACH;IACA,IAAIJ,GAAG,GAAGI,IAAI,CAACK,SAAS,CAAC,CAAC,EAAEF,QAAQ,CAAC;IACrC,IAAIM,IAAI,GAAG,IAAI,CAACd,gBAAgB,CAACC,GAAG,CAAC;IACrC,IAAG,CAACa,IAAI,EAAE;MACR,MAAM,IAAIC,KAAK,CAAC,OAAO,CAAC;IAC1B;IACA,IAAI,CAACC,SAAS,CAACf,GAAG,EAAEa,IAAI,CAAC;IACzB,IAAI,CAACjB,SAAS,CAACQ,IAAI,CAACK,SAAS,CAACF,QAAQ,CAAC,EAAEF,QAAQ,CAAC;EACpD;AACJ,CAAC;AAEDhC,SAAS,CAACW,SAAS,CAACgC,MAAM,GAAG,SAASA,MAAMA,CAAC5B,QAAQ,EAAE;EACrD,IAAIE,IAAI,GAAG,IAAI;EACfC,OAAO,CAACC,QAAQ,CAAC,YAAY;IAC3B,IAAI;MACFF,IAAI,CAACM,SAAS,CAAC,EAAE,EAAE,IAAI,CAAC;MACxBR,QAAQ,EAAE;IACZ,CAAC,CAAC,OAAMU,CAAC,EAAE;MACTV,QAAQ,CAACU,CAAC,CAAC;IACb;EACF,CAAC,CAAC;AACJ,CAAC;AAED,IAAImB,KAAK,GAAG,SAASC,MAAMA,CAAEC,OAAO,EAAEC,IAAI,EAAE;EAC1C,IAAI,CAACD,OAAO,GAAGA,OAAO;EACtB,IAAI,CAACC,IAAI,GAAGA,IAAI;EAChB,IAAI,CAAC/B,QAAQ,GAAG,YAAY;IAC1B,OAAO,IAAI,CAAC8B,OAAO,CAAC9B,QAAQ,EAAE;EAChC,CAAC;AACH,CAAC;AACDrB,IAAI,CAACe,QAAQ,CAACkC,KAAK,EAAEC,MAAM,CAAC;AAC5BD,KAAK,CAACjC,SAAS,CAACqC,OAAO,GAAG,SAASA,OAAOA,CAAA,EAAG;EAC3C,OAAO,IAAI,CAACF,OAAO;AACrB,CAAC;AAED9C,SAAS,CAACW,SAAS,CAAC+B,SAAS,GAAG,SAASA,SAASA,CAACf,GAAG,EAAEa,IAAI,EAAE;EAC1D;EACA,IAAIO,IAAI,GAAG,IAAI,CAACtC,WAAW,CAACkB,GAAG,EAAEa,IAAI,CAAC,IAAIA,IAAI,CAACO,IAAI;EACnD,IAAG,IAAI,CAACvC,QAAQ,CAACuC,IAAI,CAAC,EAAE;EACxB,IAAIE,KAAK,GAAG,IAAIL,KAAK,CAACjB,GAAG,EAAEoB,IAAI,CAAC;EAEhC,IAAI,CAACG,IAAI,CAACD,KAAK,CAAC;EAEhB,IAAI,CAACE,IAAI,CAAC,OAAO,EAAEF,KAAK,EAAEF,IAAI,CAAC;AACnC,CAAC;AAED/C,SAAS,CAACW,SAAS,CAACyC,OAAO,GAAG,SAASA,OAAOA,CAACvB,KAAK,EAAEkB,IAAI,EAAE;EACxD;EACA,IAAG,CAACA,IAAI,EAAE;IACR,IAAGM,KAAK,CAACC,OAAO,CAACzB,KAAK,CAAC,EAAE;MACvB,OAAO,IAAI,CAACuB,OAAO,CAACvB,KAAK,CAAC,CAAC,CAAC,EAAEA,KAAK,CAAC,CAAC,CAAC,CAAC;IACzC,CAAC,MACI,IAAGA,KAAK,EAAE;MACb,OAAO,IAAI,CAACuB,OAAO,CAACpD,SAAS,CAAC6B,KAAK,CAAC,CAAC;IACvC,CAAC,MACI;MACH,MAAM,IAAIY,KAAK,CAAC,yBAAyB,CAAC;IAC5C;EACF;EACA7C,MAAM,CAAC2D,EAAE,CAAE1B,KAAK,YAAY2B,MAAM,IAAM,OAAO3B,KAAK,KAAK,UAAW,CAAC;EACrEjC,MAAM,CAAC6D,KAAK,CAAC,OAAOV,IAAI,EAAE,QAAQ,CAAC;EACnC,IAAI,CAACxC,QAAQ,CAAC2C,IAAI,CAAC;IAACrB,KAAK,EAACA,KAAK;IAACkB,IAAI,EAACA;EAAI,CAAC,CAAC;AAC/C,CAAC;;AAED;AACA;AACA;AACA/C,SAAS,CAACW,SAAS,CAAC+C,MAAM,GAAG,SAASA,MAAMA,CAACC,OAAO,EAAE;EAClD,IAAG,OAAOD,MAAM,KAAK,OAAO,EAAE;IAC1B,KAAK,IAAI9B,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG+B,OAAO,CAACrC,MAAM,EAAE,EAAEM,CAAC,EAAE;MACrC,IAAI,CAAC8B,MAAM,CAACC,OAAO,CAAC/B,CAAC,CAAC,CAAC;IAC3B;IACA;EACJ;EACA,IAAI,CAACpB,QAAQ,CAACmD,OAAO,CAAC,GAAG,IAAI;AACjC,CAAC;AAEDC,MAAM,CAACC,OAAO,GAAG7D,SAAS;;AAE1B;AACAA,SAAS,CAAC8D,UAAU,GAAM,CAAC,SAAS,EAAE,YAAY,CAAC;AACnD9D,SAAS,CAAC+D,IAAI,GAAY,CAAC,OAAO,EAAE,MAAM,CAAC;AAC3C/D,SAAS,CAACgE,MAAM,GAAU,CAAC,eAAe,EAAE,QAAQ,CAAC"},"metadata":{},"sourceType":"script","externalDependencies":[]}